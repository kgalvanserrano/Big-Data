{"cells":[{"cell_type":"markdown","source":["# Building Review and User Tables\nThe review and user data files are the largest files you will be using in class and in your project.  Loading a DataFrame, selecting the needed columns, and caching the data from the JSON for either data file is slow, so this notebook instead loads the data and creates Spark tables named `reviews_without_text_table` and `user_table` based on the Parquet format that's common in Big Data.  Going forward, the tables can be used (on that cluster) the same as a temporary view. On a subsequently created cluster, the table definition will be gone, but the Parquet files used to build the tables will still exist and this notebook will rebuild them in less than a minute. \n\nSince the original code below excludes some of the fields in each set\nof JSON data files when building the tables initially, if you have already run this notebook and created the tables, but then want to include a different set of fields for either table,\nrerun the cell that builds that table, but ***temporarily*** set the corresponding variable to force the rebuilding of the table from the JSON data files to True.  Be sure after the table is rebuilt to set it back to False.\n\n<span style=\"color:red;\">Be sure to run this notebook before we cover SQL or you use the SQL Examples notebook</span>\n\nThe first time this notebook is run, it will take up to 45 minutes to build both tables from the JSON files.  To rerun if you comeback to your account should take less than a minute.\n\nAs noted above, once the tables are built, they can be used in Spark SQL queries the same as temporary views in Spark SQL queries.\n\n**NOTE:** The following cell includes imports and function definitions that are needed when running the cells further down for building the `reviews_without_text_table` and `user_table`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ba3dc88-d039-42e4-a0e4-318ca273d3cc"}}},{"cell_type":"code","source":["import pyspark.sql.functions as f\n\ndef files_exist(path):\n  '''Checks if a directory exists and contains some files.\n     If the directory exists, but contains no files,\n     then treated the same as if the directory did not exist.\n     Returns True if the file exists and contains files, \n     otherwise False\n  '''\n  files_exist = False # If the direcory containing the table files exists and is not empty, this will be set to true\n  try:\n    file_list = dbutils.fs.ls(path)\n    if len(file_list) > 0:\n      return(True)\n  except Exception:\n    pass # files-exist is still False\n  return(False) #if empty or path does not exist\n\n\ndef build_table(table_path, table_name):\n  '''Given the path to where the files for the table are, which is \n     generally under /user/hive/warehouse, and the name of the table,\n     this function builds the table.  Note that it assumes the table\n     was originally built using PARQUET.\n  '''\n  print(f\"building {table_name} from existing table files\")\n  spark.sql(f\"\"\"\n    CREATE TABLE {table_name} \n    USING PARQUET \n    LOCATION '{table_path}' \n  \"\"\")\n    \n\ndef table_summary(field_name, table_name):\n  '''Given a table name and a field to count n in the table, \n     this function generates a count of the number of records \n     and shows the first 10 rows truncated at 22 characters. \n  '''\n  spark.sql(f\"\"\"\n    SELECT COUNT({field_name}) AS record_count\n    FROM {table_name}\n  \"\"\").show()\n  # show 10 rows from the table\n  spark.sql(f\"\"\"\n    SELECT * \n    FROM {table_name} LIMIT 10 \n  \"\"\").show(truncate=22)\n\n\ndef process_table(table_name, table_path, data_path, field_name, force_rebuild, json_read_function):\n  '''This routine is called to create the tables based on the review or user data\n     The 6 parameters passed are:\n     1. table_name: string with the name of the table being created\n     2. table_path: string with the path in DBFS to where the table files in a Parquet format are located.  Usually under /user/hive/warehouse/\n     3. data_path: string with the path to the directory on DBFS where the multiple JSON files used to create the table are located\n     4. field_name: string with the name of a filed within the resulting table that will always be included and not Null\n     5. force_rebuild: a Boolean value as to whether the table should be rebuilt from the JSON files even if the table files exist\n     6. json_read_function: the name of a function that takes one parameter, the path to the JSON data, and returns the cached DataFrame that was created\n  '''\n  if ( spark.catalog._jcatalog.tableExists(table_name) and force_rebuild == False):\n    print(f\"{table_name} table exists\")\n  else:\n     # If the table files exist and the download is NOT being forced, build from the existing table files\n    if files_exist(table_path) and force_rebuild == False:\n      build_table(table_path, table_name)\n    else: # create dataframe from JSON files and save as a table\n      # delete existing table files if they exist\n      try:\n        dbutils.fs.rm(table_path, recurse=True)\n      except Exception:\n        pass #if the directory did not exist, then the files already were gone \n      # Check that the JSON files exist\n      if files_exist(data_path) == False:\n        raise Exception(f\"The directory containing the data files: {data_path} is either missing or was empty.\")\n      print(f\"building {table_name} from JSON files\")\n      df_temp = json_read_function(data_path)\n      # Does not already exist as a table, so write it out\n      df_temp.write.mode(\"overwrite\").format('parquet').saveAsTable(table_name)\n      df_temp.unpersist()\n  # Regardless of whether table was built from existing PARQUET files or JSON files, \n  # show some summary information for the table\n  table_summary(field_name, table_name)\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2326cb8c-de82-4a07-b6c1-f5c6eb6b31f7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Build the table for the review data\nThe following cell uses the functions defined in the cell above, so that must be run first.  \n\nThe code can be used in other projects to build the review table.  If different fields are desired in the table, \nthen the `create_review_dataframe` function should be edited, but if that function is changed, and the table already exists, then \nthe `FORCE_REVIEW_REBUILD` constant should ***temporarily*** be set to True.  When building the table from the JSON files, it\nwill require pproximately 25 minutes, but when rebuilding the table from the table files later (on a new cluster), it will \ntake less than 1 minute."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5e53234-0b22-4714-8e7a-16a543db797d"}}},{"cell_type":"code","source":["#If you want to force the building of the table from the JSON files, set FORCE_REVIEW_REBUILD to True.\n# Only do this if records are missing from the table, it's not able to create the table, or you want to change the fields \n# included in the table since it is time consuming.\n# AFTER RUNNING IT TO REBUILD FROM THE JSON DATA BE SURE THAT FORCE_REVIEW_REBUILD IS SET TO FALSE\nFORCE_REVIEW_REBUILD = False \nREVIEW_DATA_PATH = \"/yelp/review\"\nREVIEW_TBL_PATH = \"/user/hive/warehouse/reviews_without_text_table\"\nREVIEW_TBL_NAME = \"reviews_without_text_table\"\nREVIEW_FIELD_NAME = \"business_id\" # field from the review table that's always included and never Null\n\n\ndef create_review_dataframe(data_path):\n  df_reviews = spark.read.json(f\"{data_path}/*.bz2\").\\\n  select(\"review_id\",\"business_id\",\"user_id\",\"cool\",\"funny\",\"useful\",\"date\",\"stars\").cache()\n  return(df_reviews)\n  \n\nprocess_table(REVIEW_TBL_NAME, REVIEW_TBL_PATH, REVIEW_DATA_PATH, REVIEW_FIELD_NAME, FORCE_REVIEW_REBUILD, create_review_dataframe)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ffe5672-ea7f-42ac-8986-92ebf7717013"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Build the table for the user data\nThe following cell uses the functions defined in the first code cell above, so that must be run first.  \n\nThe code can be used in other projects to build the user table.  If different fields are desired in the table, \nthen the `create_user_dataframe` function should be edited, but if that function is changed, and the table already exists, then \nthe `FORCE_USER_REBUILD` constant should ***temporarily*** be set to True.  When building the table from the JSON files, it\nwill require pproximately 18 minutes, but when rebuilding the table from the table files later (on a new cluster), it will \ntake less than 1 minute."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d03e776-db5e-4349-a068-f0cfc4d1ecc3"}}},{"cell_type":"code","source":["FORCE_USER_REBUILD = False \nUSER_DATA_PATH = \"/yelp/user\"\nUSER_TBL_PATH = \"/user/hive/warehouse/user_table\"\nUSER_TBL_NAME = \"user_table\"\nUSER_FIELD_NAME = \"user_id\" # field from the user table that's always included and never Null\n\n\ndef create_user_dataframe(data_path):\n  df_users = spark.read.json(f\"{USER_DATA_PATH}/*.bz2\").\\\n  withColumn(\"friend_count\",f.size(f.split(f.col(\"friends\"),'\\s*,\\s*'))).drop(\"friends\").cache()\n  return(df_users)\n  \n\nprocess_table(USER_TBL_NAME, USER_TBL_PATH, USER_DATA_PATH, USER_FIELD_NAME, FORCE_USER_REBUILD, create_user_dataframe)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ffd4952-e844-431f-ad94-fea9520fa89f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Building Review and User Tables","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3238669313265070}},"nbformat":4,"nbformat_minor":0}
