{"cells":[{"cell_type":"markdown","source":["Copyright Scott Jensen, San Jose State University\n\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">This notebook</span> by <span xmlns:cc=\"http://creativecommons.org/ns#\" property=\"cc:attributionName\">Scott Jensen,Ph.D.</span> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14693142-6922-4fd2-a4fa-3a3743e40341"}}},{"cell_type":"markdown","source":["# Working With Files Part 1: Loading The Yelp Data\n\nAlthough the Yelp datset is not \"Big\" by commercial standards, for an academic dataset it's large in that unzipped it's approximately 10GB. Fortunately, Spark can work directly with compressed data in certain formats, and we will be loading zipped data using the bzip2 format (WinZip files will not work - don't try it).\n\nSome of the data files, particularly the reviews and the user data files, are nearly 2GB even when compressed, so loading them from a home Internet connection is not possible for many students (keep in mind that if your ISP is a cable company, data download speeds are usually much faster than data upload speeds, and you would need to do both).  If you are curious about your Internet speed, see the <a href=\"https://www.att.com/support/speedtest/\" target=\"_blank\">AT&T speedtest</a> (there's also a link in Canvas) - you would have roughly 900 Mbps both directions when using a wired Ethernet (not Wi-Fi) connection on campus.\n\nFor this reason, we staged the zipped data on Amazon's S3 storage service and we will use the code in this notebook to load the data directly to your Databricks account (which is also on AWS's servers).  The code is designed so that if you re-run this code and the data is already in your account, it will not try to reload it.  This is also why you completed the Yelp dataset agreement exercise where you submitted the request to Yelp and agreed to be bound by their license.\n\nWe will walk through this notebook in class. Since the review and user files are rather large  (almost 2GB each when compressed), you will also need to run the notebook named `Building Review and User Tables` to create tables for the review and user data files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62ee669e-03c8-48a4-b5fc-c4636f04056c"}}},{"cell_type":"markdown","source":["### Step 1: Spinning up a cluster\nTo be able to calculate any cells in your notebook, you will need to be attached to a cluster.  If you completed the exercise in the *Intro to Jupyter and Python* video lecture, this is the same process.  From the toolbar on the\nleft-hand side, click on the `Compute` icon (it looks like a cloud and used to be amed Clusters).  You won't have any clusters to start with, so click on the `+Create Cluster` button.\nThe defaults in that screen are fine, but you will need to enter a name for your cluster.  Once your cluster's status is \nrunning (and has a solid green circle by it), come back to your notebook and from the drop-down list in the top-left corner, attach \nyour notebook to the cluster listed with the solid green circle."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe86edb4-059e-4522-bdb5-d7e07b9c40b5"}}},{"cell_type":"markdown","source":["### Step 2: Using a Databricks widget to enter the path to the data manifest\n\nSince some of the data files are too large to manually upload even when compressed, you will be importing the data from where we temporarily staged it on AWS, but **you *MUST* complete the dataset agreement assignment in order to earn credit for *ANY* of the exercises or the team assignments which use the Yelp data**.  To bring the data from where we staged it on AWS to your Databricks account hosted on AWS (for free by Databricks! Yay!) the code below needs to know where it can downlaod the data from.\n\nWhen you run the next cell, a \"widget\" will appear **at the top of the notebook** that prompts you for your the path to a data manifest.  \nOn the right-hand side of that widget bar you will see a pushpin (a.k.a. thumbtack) icon that will allow you to pin that to the top of the code window even when you scroll down.  Once you have entered the manifest's path, you can \"unpin\" the widget bar to make more screen real estate available.\n\nWe will be importing three files to your Databricks account and eventually we will be putting the compressed files they contain in a directory named `/yelp` on the Databricks File System (DBFS) on your Databricks account. The manifest uses a JSON format and contains a JSON array with an object for each file.  Each JSON object provides three properties: the name of the file, an MD5 sum for the file, and a flag as to whether the file should be unzipped. The MD5 sum is a one-way hash that allows us to make sure the file download did not encounter any errors and is exactly the same as the file we originally staged on AWS.  The flag for whether to unzip is because we compress the data flies using the bzip2 format, but the review and user data files have been split by year (the year of the review or the year a user joined Yelp), so we zipped up the directories containing those files.  We need to unzip those directories before moving the files to the `/yelp` irectory on DBFS. You may be wondering why the manifest file is used, but it allows us to easily move the files or update them, and we only need to provide you with the URL to where you can find the current manifest.\n\nTo see the widget (if you just imported the notebook), click on the arrow in the upper \nright-hand corner of the next code cell and select `Run Cell`.\n\n#### The following cell is Step 2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50f8c16a-44d2-4698-abc0-bdfad64a49c3"}}},{"cell_type":"code","source":["dbutils.widgets.text(\"manifest_url\",\"xxxxxxxxxxxxxxx\",\"Enter the manifest URL:\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba7c79b4-5db5-4afc-b0cf-780bb1e9df9b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Step 3: Entering the file manifest URL\n\nIn the input box for the widget with the prompt \"Enter the manifest URL\", enter the URL we provide in class.\n\nOnce you have entered the URL in the widget, we can get started."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14e680d6-5e6e-468e-8893-9d8d6ae9abc3"}}},{"cell_type":"markdown","source":["### Step 4: Creating a directory for your data files using the DBFS utilities\n\nYou are going to be running code to bring your data over from our bucket on Amazon's S3 to your account on Databricks.  As discussed previously, your data is stored in the Databricks File System, which is referred to as DBFS.  To be able to store your data in DBFS, you first need to create a directory in which to store the data, we are going to name that directory \"Yelp\" (but without the quotation marks).\n\nThe commands for working with DBFS are very similar to Linux, but if you have not used Linux, don't panic, the number of commands we will be using can be counted on one hand (unless you are ET - he only had 2 fingers, but we are assuming you are an Earthling).  If you are a Mac user and have played around at the command line on your laptop, these commands will look familiar since the operating system for a Mac is a variant of Unix.  All of the commands for working with DBFS are in the dbutils library (which Databricks has conveniently already installed in your account when you spun up a cluster).  Although we will be using only a few commands, you don't need to memorize them, just run the following command in a blank cell and it will show you all of the available commands:\n\n`dbutils.fs.help()`\n\nThat will display each file system method along with the syntax and a brief description.\n\nThe `dbutils` library contains Databricks utilities other than those for working with files, so we are calling the file system, or `fs`, set of methods within `dbutils`.  \n\nThe `dbutils.fs` methods are divided into file system utilities (fsutils) and mount methods.  You won't be mounting other drives, so you will only be using the fsutils methods. Run the next cell to see the syntax for the file commands."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"846afd03-e68a-4820-ae6c-a0f89cde7398"}}},{"cell_type":"code","source":["dbutils.fs.help()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"915f6b57-d204-4a85-8557-50c0560cbac9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Step 4 continued ...\nIf you read the list of file utility methods, you will see that the method for creating a directory is named `mkdirs` and takes one parameter, a string with the path and name of the directory we want to create.  Since we are passing a string parameter, we enclose the text in quotes.  These can be single or double-quotes, but they need to match.\n\nYou may be wondering why the path starts with a slash.  If you are a Mac user familiar with the command line, this would seem natural, but if you are a PC user it may not.\nThe slash indicates the root.  In Windows, you specify a path by saying the drive letter, but in Linux (and on the Mac) there are no drive letters - drives are mounted as paths.  In DBFS, as in Linux, on the Mac, and for paths in URLs on the Internet, \nthe separators for directories or folders on a path are forward slashes instead of the backslashes used in Windows.\n\nRun the next cell to create your yelp directory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f57224e0-f5fd-4f01-9cfb-4adc8bc27545"}}},{"cell_type":"code","source":["dbutils.fs.mkdirs(\"/yelp\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9af2a111-f9d7-4212-9f05-74c3169ebab8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### **<span style=\"color:#22b922\">Riddle me this</span>**: Why does the output have the value `True`?  \n\nThe `mkdirs` method has a Boolean return value, so instead we could have run the following code:\n\n`result = dbutils.fs.mkdirs(\"/yelp\")`\n\nThat would have created a new variable named `result` and assigned the value returned by `mkdirs` to that variable.  The value assigned to `result` would have been the Boolean value `True` (assuming the directory was created)\nand then we could have printed the value of `result` with the following Python code:\n\n`print(result)`\n\nIn the code we actually ran, we did not assign the value returned by the mkdirs method to a variable, so Jupyter printed out the value returned by the method call.\n\n**PLEASE NOTE:** Although Jupyter printed out the returned value of our call to `mkdirs`, this is only because it's the last line in our code cell.  If we had additional code in that cell that wrote out or generated other values, the `True` returned by the call to `mkdirs` would not be shown."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6554eeb-611f-426e-9ed0-fea877107fb6"}}},{"cell_type":"markdown","source":["### Step 5: Importing the data\n\nThe code in the following cell will import the data files to your yelp directory.  While that cell is running (**it will probably take around 5 minutes**), let's talk about what is happening in that cell.\n<ol>\n  <li style=\"padding-bottom:5px;\">First, we are importing the library named `requests` which allows us to retrieve data over the web.  This is not built into the core of Python, so it's a separate library.  However, since it's one that's commonly used, Databricks has it in your cluster already.  If this was a less commonly used library, you would need to first load the library on your cluster.</li>\n  \n  <li style=\"padding-bottom:5px;\">We define a few variables, such as the part of the URL that's the same for all files located on AWS, where we will be downloading the data from.</li>\n\n  <li style=\"padding-bottom:5px;\">To make the process easier to understand, we break it into steps that can be defined as separate functions.  Each function is defined starting with `def` and the indenting in Python tells it when each function has ended.  The main code near the end of the cell is less than 10 lines long and calls the functions defined above.</li>\n\n  <li style=\"padding-bottom:5px;\">One of the first functions called is `get_manifest`, which is passed the URL you entered for the widget at the top of the notebook, it tries to download the manifest from that URL, and the manifest says what files are being downloaded.  The manifest is a tiny JSON file that we store out at a public URL and that file is a JSON object containing a name:value pair with the id for the bucket where the data is stored, and two name:value pairs with arrays as the values:\n    <ul>\n      <li style=\"padding-bottom:3px;\">An array of the files or directories that will be created in the directory where we download the data.</li>\n      <li style=\"padding-bottom:3px;\">The second array contains a JSON object for each download file with:\n        <ol>\n          <li>The name of the file being downloaded</li>\n          <li>The MD5 sum of the file. The MD5 sum is a hash of the file contents that returns a string - this is used to make sure the file downloaded is complete.</li>\n          <li>A flag indicating if the downladed file should be unzipped.  The review and user downloads are zipped files containing a directory of files.</li>\n        </ol>\n      </li>\n    </ul>\n  </li>\n  <li style=\"padding-bottom:5px;\">The code cycles through downloading and unzipping the files (as needed), and in our case the user and review files contain a directory of files compressed using the bzip2 format which Spark can load.  As each file is downloaded, the MD5 sum of the file is calculated and compared to the MD5 sum in the manifest to make sure the download is complete and data is not missing.</li>\n</ol>  \n\nWhen downloading the data, we cannot work directly in DBFS, so we download and unzip the data using the driver node and then movng the data to DBFS.  Keep in mind that where we download the files\non the file system local to the driver node of the cluster disappears when the cluster terminates, but the files we move to DBFS are permanent and will be there again when you start a new cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d118f5b7-ca8e-4ace-b1f7-17a5a3a8a93d"}}},{"cell_type":"code","source":["import pyspark.sql.functions as f\nimport requests\nimport re\nimport tarfile\nimport zipfile\nimport json\nimport hashlib\n\nFORCE_DOWNLOAD = False\n\nURL_HOST = \".s3-us-west-2.amazonaws.com/\"\nTEMP_DIR = \"/yelptemp/\"\nZIPPED_DIR = \"/yelptemp/zipped/\"\nUNZIPPED_DIR = \"/yelptemp/unzipped/\"\nDATA_DIR = \"/yelp\"\n\ndef clean_all():\n  ' Removes the data directory if it exists on DBFS'\n  try:\n    dbutils.fs.rm(DATA_DIR,recurse=True)\n  except Exception:\n    pass\n  \n\ndef get_manifest():\n  ''' Returns the dictionary that's the download manifest based on the URL\n      entered in the URL widget.\n      If it's not a valid URL or returns a status code other than 200, an exception is raised.\n      If the manifest is not valid JSON, or does not contain name:value pairs named\n      id, data_list, and download_list, an exception is raised.  \n  ''' \n  manifest_url = dbutils.widgets.get(\"manifest_url\")\n  response = requests.get(manifest_url)\n  if response.status_code != 200:\n    raise Exception(f\"The manifest URL {manifest_url} returned a status of \" + str(response.status))\n  manifest = response.text\n  try:\n    manifest_dict = json.loads(manifest)\n    # The manifest should have a data_list element and a down_Load list element\n    if \"data_list\" in manifest_dict == False:\n      raise Exception(\"The manifest does not contain a data_list.\")\n    if \"download_list\" in manifest_dict == False:\n      raise Exception(\"The manifest does not contain a download_list.\")\n    return(manifest_dict)\n  except json.JSONDecodeError as err:\n    raise Exception(\"The manifest is not a valid JSON document.\", err)\n    \n\ndef check_data(manifest):\n  ''' Function used to check if the data directory contains valid\n      copies of all of the files in the download. The manifest dictionary\n      is passed as a parameter and is expected to comtain a data_list containing\n      the names of each file expected in the data directory.\n      If a file is missing, this method returns False.\n      If all of the files exist, it returns True.\n  '''\n  try:\n    data_dir_list = dbutils.fs.ls(DATA_DIR)\n    if len(db_dir_list) == 0:\n      return(False)\n      file_list = manifest[\"file_list\"]\n      existing_list = dbutils.fs.ls(DATA_DIR)\n      for file_name in file_list:\n        found == False\n        for info in existing_list:\n          if info.name == file_name:\n            found == True\n            break\n        if found == False:\n          return(False)\n      # looped through all of the required files and they are there\n      return(True)\n  except Exception:\n    # The directory does not exist, does not match the manifest, or the hashes don't match\n    return(False)\n  \ndef get_bucket_id(manifest):\n  ''' The manifest is expected to contain a name:value pair named id\n      where the value is the bucket name on S3 where the files are\n      staged.  If the id is missing or is a blanks string, then an\n      exception is raised, otherwise the bucket id is returned.\n  '''\n  try:\n    bucket = manifest['id'].strip()\n    if len(bucket) == 0:\n      raise Exception(\"The id provided in the manifest was an empty string, but should be the name of the bucket being downloaded from.\")\n    else:\n      return(bucket)\n  except Exception as e:\n    raise Exception(\"An error occurred in retrieving the bucket id from the manifest\", e)\n      \n  \ndef download_file(manifest_item, bucket_id):\n  ''' Given a dictionary from the download list, download the file to the\n      temporary directory for downloading the file and check the\n      MD5 sum to make sure it matches.\n      If the MD5 sum does not match, an excetion is raised, otherwise it prints\n      that the file was successfully downloaded.\n  '''\n  file_name = manifest_item[\"name\"]\n  item_md5sum = manifest_item[\"md5\"]\n  request_url = \"https://\" + bucket_id + URL_HOST + file_name\n  local_name = ZIPPED_DIR + file_name \n  print(\"requesting file from:\", request_url)\n  r = requests.get(request_url, stream=True)\n  status_code = r.status_code\n  # If the status code is 200, then we successfully retrieved the file\n  if status_code != 200:\n    raise Exception(f\"The {file_name} download failed. A status code of {str(status_code)} was returned from the URL:{request_url}.\")\n  else: # write the file \n    with open(local_name, 'wb') as file:\n      for chunk in r.iter_content(chunk_size=4096):\n        file.write(chunk)\n        file.flush()\n    file.close()\n  #check if the hash of the file downloaded matches the md5 sum in the manifest\n  with open(local_name, 'rb') as data_file:\n    md5sum = hashlib.md5( data_file.read() ).hexdigest()\n    if md5sum.lower() != item_md5sum.lower():\n      raise Exception(f\"The file {file_name} downloaded from Google Drive generated a MD5 sum of {md5sum} instead of the MD5 sum in the manifest ({item_md5sum}) so it may be corrupted and the processing was terminated.\")\n    else:\n      print (\"successfully downloaded:\", file_name)\n\n      \ndef process_file(manifest_item):\n    ''' The file is now downloaded.  If the file is zipped,\n        it first needs to be unziiped, and either way, moved\n        to the DBFS data directory.\n    '''\n    local_name = ZIPPED_DIR + manifest_item[\"name\"]\n    local_path = \"file:\" + local_name\n    is_zipped = manifest_item[\"zipped\"] == \"true\" # This is either Ture or False\n    if is_zipped:\n      with zipfile.ZipFile(local_name,\"r\") as zip_ref:\n        zip_ref.extractall(UNZIPPED_DIR)\n      untar_info = dbutils.fs.ls(\"file:\" + UNZIPPED_DIR)\n      # The zip file could contain a directory, a file, or more than 1 file,\n      # so we loop through the file list, moving all of them to DBFS\n      for info in untar_info:\n        destination = DATA_DIR + \"/\" + info.name\n        dbutils.fs.mv(info.path, destination, recurse=True)  \n      dbutils.fs.rm(local_path)\n    else: # file was not zipped (or should remain zipped), so just move it\n        destination = DATA_DIR + \"/\" + manifest_item[\"name\"]\n        dbutils.fs.mv(local_path, destination)  \n    print (\"processed:\", local_name)\n    \n                      \ndef load_data(manifest_list, bucket_id):\n  ''' Loops through the files in the download list from the manifest and \n      downloads the file, verifies the MD5 sum is correct, unzips it if needed,  \n      and moves the file or folder that was in it to the data directory.'''\n  # Create the empty temporary directories\n  try:\n    dbutils.fs.rm(\"file:\" + TEMP_DIR,recurse=True)\n  except Exception:\n    pass\n  # Create the temporary local directory and sub-directories\n  dbutils.fs.mkdirs(\"file:\" + TEMP_DIR)\n  dbutils.fs.mkdirs(\"file:\" + ZIPPED_DIR)\n  dbutils.fs.mkdirs(\"file:\" + UNZIPPED_DIR)\n  # Loop through the files to download\n  for item in manifest_list:\n    download_file(item, bucket_id)\n    process_file(item)\n  # Remove the temp directory used to unzip the files\n  dbutils.fs.rm(\"file:\" + TEMP_DIR, recurse=True)\n  \n  \n# *******************************************  \n# Run the Actual Routine to Load the Data\n# This code uses the above defined functions\n# *******************************************\nif FORCE_DOWNLOAD == True:\n  clean_all()\nmanifest_dict = get_manifest()\nif check_data(manifest_dict) == False:\n  bucket_id = get_bucket_id(manifest_dict)\n  download_list = manifest_dict[\"download_list\"]\n  load_data(download_list, bucket_id)\nelse:\n  print(\"All of the required files exist in the data directory already, so the download was not processed.\")\nprint(\"Done\")\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"319fca6a-ed9a-41dc-b655-c765bcff85b7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Step 6: Listing the Files Loaded\n\nIn the above cell that brought the data over to your Databricks account, we use the dbutils method to list the file contents of the directory.  Here you are going to use that utility again, but since it will be the only line in your cell (so it's the last line), you don't need to assign the value returned to a variable, the result return will be printed as the output of the cell (like the `true` value returned when you created a directory).\n\nIn the following cell, add a line of code to list the `/yelp` directory.  You should see three files and two directories listed.\n\nIf we wanted to *use* the list returned by the `ls` method, we could assign it to a variable name."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"412ca284-2efa-4273-b084-ab6814eed72b"}}},{"cell_type":"code","source":["# Add your code here and run it to list the contents of your /yelp directory on DBFS\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"072b540a-b819-45aa-bf61-28954a2b8519"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Step 7: Listing the Review and User Subdirectories\nIn the above cell when you listed the contents of the `/yelp` directiory on DBFS, both `review` and `user` had a size of zero and ended with a slash, indicating they were directories and not files.  Copy the code you used in the above cell to list the `/yelp` directory (copy it twice actually), and modify the path passed as a parameter so that it first lists the contents of the `review` directory and then the contents of the `user` directory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"631e5fdd-9020-48a7-8ce4-4e0f564e64f1"}}},{"cell_type":"code","source":["# Add code here and run it to list the contents of the /yelp/review director on DBFS\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f33875c-b4bc-4bcc-89c0-729219136c47"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Add code here and run it to list the contents of the /yelp/user directory on DBFS\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b699d66-8b4c-498c-8475-41fa47c73cad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Working With Files - Part 2: Getting the Category Definitions\n\nAbove you loaded the Yelp data as zipped files in the bzip2 format to save space.  In the following cell we will take a slightly different approach using the `urllib` module to read a JSON data file from a page on Yelp's website.  \n\nIn the dataset's business file, most businesses have a `categories` field which is a comma-separated list of the categories in which a business operates.  Some categories are at a high level (such as \"Restaurants\"), but the categories form a hierarchy with increasing levels of detail, so there are more specific categories too, such as \"Dim Sum\" which is within the \"Chinese\" category, which in turn is within the \"Restaurants\" category.  There are over 1500 categories (and growing).  As part of their \"Fusion API\", Yelp makes this list available to web developers who are creating apps that use Yelp data (and drive traffic to Yelp).  The page documenting the controlled vocabulary for categories can be found at the following website: <a href=\"https://www.yelp.com/developers/documentation/v3/all_category_list\" target=\"_blank\">https://www.yelp.com/developers/documentation/v3/all_category_list</a>.\n\nOn that site there's a link to a JSON file defining the hierarchy for this controlled vocabulary.  Although there are a lot of categories, as a JSON file this file is tiny compared to the Yelp data, so we don't need to compress it.\n\n### Step 1: Run the code in the following cell to load the data (or see below for an alternate approach)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76b35ced-484b-431b-9163-5a910e767554"}}},{"cell_type":"code","source":["import urllib\nimport json\n\nTEMP_DIR = \"/bus4118d/\"\nDBFS_DIR = \"/yelp/\"\nCAT_FILENAME = \"categories.json\"\nCAT_URL = \"https://www.yelp.com/developers/documentation/v3/all_category_list/categories.json\"\n\n# create the temp directory for downloading and the DBFS directory if they don't exist\ndbutils.fs.mkdirs(\"file:\"+TEMP_DIR)\ndbutils.fs.mkdirs(\"dbfs:\"+DBFS_DIR)\n# If the file already exists, we will remove it\nlocal_name = \"file:\"+TEMP_DIR+CAT_FILENAME\ndbutils.fs.rm(local_name)\ndbfs_name = \"dbfs:\"+DBFS_DIR+CAT_FILENAME\ndbutils.fs.rm(dbfs_name)\n\n# get the file and store it in the temp directory\n# Note that with the request, the destination \n# directory is inherently on the local driver\nurllib.request.urlretrieve(CAT_URL, TEMP_DIR+CAT_FILENAME)\n# move the file to DBFS\ndbutils.fs.mv(local_name, dbfs_name)\ndbutils.fs.ls(DBFS_DIR)\n# Create and show a DataFrame\ndf_categories = spark.read.json(DBFS_DIR+CAT_FILENAME, multiLine=True)\nprint(\"categories:\",df_categories.count())\ndf_categories.show(50, truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9c6a749-e286-4f08-b488-c33aa634fee9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Category Definitions - Alternate Approach\nThe categories.json file is a small file, so we could also load it through the GUI interface, and that's what we will do in this alternate approach.\n\nTo load the categories.json through the GUI, see the lecture slides for the class.  The slides walk through the following steps:\n1. Download the categories.zip file form the Canvas module for this week and unzip the file.  You should now have a file named categories.json\n2. Through the GUI on the Data option (click the Create Table button, then load the file, but don't create a table)\n\nYou will now have a file on the path: /FileStore/tables/categories.json\n\nYou want to move that file to the path: /yelp/categories.json\n\n#### Step 2a: Use the `mv` method from file system methods in dbutils to move the file"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acf7bf22-329d-4a51-8372-19eb856a7fdc"}}},{"cell_type":"code","source":["# Add your code to move the file in this cell\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0248ad4-4a6c-4f9e-b8c2-7bd55c07036e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Step 2b: List the files in /yelp\n\nYou have now moved the categories.json file into the same directory where you loaded the Yelp data above in Part 1. In the following cell, list the files again and you will now see the Yelp data and the categories.json file."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eea956f3-018c-48b4-9ecf-9d0cd54c8070"}}},{"cell_type":"code","source":["# Add your code to list the directory in this cell\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9bace0e9-c187-43a8-a664-05009115aa74"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Working with Files - Part 3: Finding Gender Data\n\nYelp wants to create the best user experience possible and show authentic reviews.  They have a proprietary \nalgorithm for ranking the reviews they show users.  The average star rating for a business is part of it,\nbut not the whole story.  The number of reviews is part of it, but not the whole story.  \nSince many users will not read more than a couple reviews, having a good algorithm when ordering the reviews to \nshow them to a user is critical to Yelp's business.  They need to always be thinking of how to make the ranking better\nin order to improve the customer experience.\n\nWhat if men and women review differently?  Is a 4-star rating from a man the same as a 4-star rating from a woman?\nIn other words, might men or women consistently rate businesses higher or lower?  Would this depend on the type\nof business?  Are ratings by one gender more consistent than the other?  If so, could we be more certain of the validity\nof the ranking of a business based on 5 reviews by women than we would by 5 reviews by men (or vice versa)?\n\nIf there is a difference, should that be taken into account when Yelp ranks businesses based on their ratings?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32f51804-2be4-4017-8d61-68f52c63e37c"}}},{"cell_type":"markdown","source":["### Using First Name as a Proxy for Gender\n\nWe have a problem.  We don't have information about the user's gender.  However, we are curious and persistent.  Is there a proxy\nwe could use?  A proxy is a stand-in or substitute for something else.  Could a user's first name be a proxy for their gender?  What issues would we have if we use name\nas a proxy for gender?\n\nFirst, we need some data to associate user names with genders.  We can start searching on the web.  Possibly lists of baby names?\nIf you were to search for a while, you would find the Social Security Administration's (SSA) website with the 1000 most popular baby names\nfor girls and boys (at least in the U.S.), but we want more than the most popular names, we want to tie as many names as possible to a gender.\nIf you dig a little further, you'll find the SSA page titled <a href=\"https://www.ssa.gov/oact/babynames/limits.html\" target=\"_blank\">Beyond the Top 1000 Names</a>.\n\nRead that page - they have a zip file there with national data as to every first name used to apply for a social security account and a count\nof the number of men and women applying with that name, based on their date of birth.  Hover your mouse over that link, the file can be downloaded \nfrom the following URL:  <a href=\"https://www.ssa.gov/oact/babynames/names.zip\" target=\"_blank\">https://www.ssa.gov/oact/babynames/names.zip</a>\n\nWe could download that file and unzip it (and you may want to do that after class), but what the zip file contains is a file for each year-of-birth, so \nfor those little girls and boys born in 2017 who applied for a social security card, the file is named `yob2017.txt` (a copy is in this week's module in Canvas) and it contains data in the following\nformat:\n\nEmma,F,19738 <br/>\nIsabella,F,15100 <br/>\nSophia,F,14831 <br/>\nMia,F,13437 <br/>\nLiam,M,18728 <br/>\nLogan,M,13974 <br/>\nBenjamin,M,13733\n\nSo if you just had a baby boy or girl and named her Isabella or named him Liam because you thought it would be unique, apparently so did everybody else.\n\nAs you can see, the file has 3 columns, the name, the gender (M or F), and the number of people with that name who were born in 2017 and applied for a social security card.  You should also note there are no headers.\n\nIt does not say what year the data is from, but the year is in the name of each file, so in a later exercise we will do an *enriching transformation* to insert\nthat metadata into a new column in the DataFrame.\n\n### Step 1: Downloading the data\n\nWe are again going to use the `urllib` module. As before with the Yelp categories data, for the download, all of the necessary code is already included below.\n\nUsing `dbutils.fs`, we will create a new directory named `ssa` where we are going to download the zip file from the SSA: `names.zip`.  However, we use the `file:` prefix for the path to say we want to create the directory local to the driver node for our cluster.  We need to do this because we cannot treat DBFS as a local file system.  Later we will move the zipped and unzipped data to DBFS."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0467fbb-4f30-4881-91ea-e9f45db947cb"}}},{"cell_type":"code","source":["import requests\nimport urllib\n\nSSA_URL = \"https://www.ssa.gov/oact/babynames/names.zip\"\nSSA_DIR =\"/ssa/\"\nSSA_FILENAME = \"names.zip\"\n\n# If the ssa directory exists, remove it\ndbutils.fs.rm(\"file:\"+SSA_DIR, recurse=True)\ndbutils.fs.mkdirs(\"file:\"+SSA_DIR)\n\nurllib.request.urlretrieve(SSA_URL, SSA_DIR+SSA_FILENAME)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9bec4a77-ee12-430e-bdb2-a7663d4a9ab6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["###  Step 2: Check if your file was written\n\nDid the file end up in our `ssa` directory?  In the cell below, add code that uses the `ls` method from  `dbutils.fs` to list \nthe contents of the `file:/ssa` directory which is a directory local to the driver node and not DBFS."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a55c018-77c3-4c30-8057-afdaf343ae75"}}},{"cell_type":"code","source":["# Add your code here to list the /ssa directory (and run the code)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ce6202b-b848-4c94-8f1c-2c506ff79e94"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Step 3: Unzipping the data\nWhen we loaded our Yelp data we loaded bzip2 files because Spark does not read zip files, but here we\nare still using Python (not Spark), to unzip the file, so that's not a problem.\n\nWe will need to import another Python module that has the functions to unzip a file, and we will\nuse the Databricks `dbutils` functions to create a subdirectory we are going to unzip the data files into. \n\nKeep in mind that when we are running the zip commands, we are just using Python and not Spark.  Why does that matter?  The paths we provide\nare not pointing to paths on DBFS (since the plain old Python does not \"know\" about DBFS).  When we use the path \"/ssa\", the zip command assumes\nwe are talking about the `ssa` directory off the root of the driver node.  If we use the path \"/ssa\" in a dbutils command, it assumes we are\nreferring to a DBFS path since we did not prefix the path with `file:`.\n\nOnce we unzip the data, we must move the directory to DBFS.  While data on DBFS will remain when our cluster shuts down, the files \nlocal to the driver will not exist once the cluster is shutdown.\n\n**After running the following cell, in Step 4 be sure to use the `ls` method to see the listing of all of the SSA files for each year**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96bb14ef-fc16-4ac4-8101-bfc4b507ec94"}}},{"cell_type":"code","source":["import zipfile\n\nSSA_SUBDIR = \"data\"\nssaDataDir = \"file:\" + SSA_DIR + SSA_SUBDIR\ndbutils.fs.mkdirs(ssaDataDir) \n\nssaNamesZip = SSA_DIR + SSA_FILENAME\n\nwith zipfile.ZipFile(ssaNamesZip,\"r\") as zip_ref:\n    zip_ref.extractall(SSA_DIR + SSA_SUBDIR)\n    \n# Move the files to DBFS\ndbutils.fs.rm(\"dbfs:\"+SSA_DIR, recurse=True)\ndbutils.fs.mv(\"file:\"+SSA_DIR, \"dbfs:\"+SSA_DIR, recurse=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd6fa412-1291-4dcb-9926-0dc480e834c8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["###  Step 4: Check if your data was unzipped properly\n\nDid the files for each year's data end up in our `data` subdirectory under `ssa`?  Use the `ls` method from  `dbutils.fs` to list \nthe contents of the `/ssa/data` directory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53a15dff-2981-4c36-9b55-053785bac7f3"}}},{"cell_type":"code","source":["# Add your code here to list the contents of the /ssa/data directory (and run it)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4c0052d-6a4c-4f08-a4de-7b7ceb90491d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Step 5: What does one of these data files look like?\n\nUsing the `head` method in `dbutils.fs` we can take a peek at the first \"X\" bytes.  We will use the default of 64K."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8696efb0-1818-418d-b93b-9d7340b4ab41"}}},{"cell_type":"code","source":["dbutils.fs.head(\"dbfs:/ssa/data/yob1880.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6805978-4b1d-411c-9ccf-201d62a0ba59"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Step 6: Making it more human-readable\nIn the above command, the `head` function is just getting bytes.  For a more human-readable view of the head of the file, we can enclose the call to `dbustils.fs.head()` inside a call \nth the Python print function and the `\\r\\n` which represent line feeds in the file will now show each name on a separate line.\n\n**In the following cell**, enclose the call to the `head` function from the cell above in a call to the python `print` function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da470a38-6996-4b76-b456-9f0ca2d57d82"}}},{"cell_type":"code","source":["# In this cell, add a call to the print function around the call to the dbutils.fs.head method\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73401e05-2b60-4110-8eec-cd95db4e7145"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Assignment Deliverable\n\n* Make sure you have added and run code for those steps where you were supposed to list or print the results.\n* Publish your notebook as described in the lecture video titled *Intro to Jupyter and Python* \n* Submit the published URL as the deliverable for this assignment"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ed8342b-611a-4a34-997d-825d731299b1"}}}],"metadata":{"name":"workshop_master","notebookId":"823144322187040","application/vnd.databricks.v1+notebook":{"notebookName":"Working With Files Version 2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2222704103242821}},"nbformat":4,"nbformat_minor":0}
